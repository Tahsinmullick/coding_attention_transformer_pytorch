{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Self-Attention Class using Pytorch to understand the math behind Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Transformer models depend on **Attention** mechanism to make next predictions. Today's **Large Language Models** depend on Attention mechanisms to yield useful outputs. The **Self-Attention** mechanism is one type of Attention. In which each word in a sentence along with their encoding builds **contextual awareness** through **Attention scores** that helps to develop an understanding of **associations** between words in a sentence. This happens with not only **neighboring words** but also with **itself**. Hence this type of Attention is called **Self-Attention**. \n",
    "\n",
    "**Content:**\n",
    "- **[Self-Attention Class](#SAttention)** Self Attention class helps transformer to form relationship among words and tokens.\n",
    "\n",
    "- **[Method to calculate Self-Attention value](#attention_values)** Using Self-Attention class to calculate Self-Attention scores for sample data\n",
    "\n",
    "- **[Test with more examples](#validate)** Verifying the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn # torch.nn yields the nn and \n",
    "import torch.nn.functional as F # For SoftMax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to code the selfAttention class which will output Attention Scores based on the following  equation $Attention(Q, K, V) = SoftMax(\\frac{Q.K^{T}}{\\sqrt{d_{K}}}).V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class inherits from Module\n",
    "class SelfAttention(nn.Module):\n",
    "\n",
    "    # d_model = The number of embedding values per token\n",
    "    # [NOTE:\"Attention Is All You Need\" d_model was set to 512]\n",
    "    #  row_dim: indices to access rows\n",
    "    #  col_dim: indices to access columns\n",
    "    def __init__(self, d_model=2,\n",
    "                 row_dim = 0,\n",
    "                 col_dim = 1):\n",
    "        \n",
    "        # Inherit from Module class\n",
    "        super().__init__()\n",
    "\n",
    "        # Intializing the weigth matrix of Query, Value and Key respectively\n",
    "        # for each token.\n",
    "        # [Note: Though alot of implementations include bias terms when creating queries, \n",
    "        # keys, and values. The orginial \"Attnetion is all we need\" did not use it.\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False) \n",
    "\n",
    "        self.row_dim = row_dim\n",
    "        self.col_dim = col_dim\n",
    "\n",
    "    def forward(self, token_encoding):\n",
    "        # creating query, key and value using encoding numbers\n",
    "        # associated with each token (token encodings)\n",
    "\n",
    "        q = self.W_q(token_encoding)\n",
    "        k = self.W_k(token_encoding)\n",
    "        v = self.W_v(token_encoding)\n",
    "\n",
    "        # computing similarity score (q * K^T) i.e. dot product\n",
    "        sims = torch.matmul(q, k.transpose(dim0 = self.row_dim, dim1 = self.col_dim))\n",
    "\n",
    "        # scaling similarity score by dividing by sqrt(k.col_dim)\n",
    "        scaled_sims = sims/ torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        # performing SoftMax on the scaled similarity scores\n",
    "        # to determine percent of each tokens' value to\n",
    "        # use in the final attention values.\n",
    "        attention_percents = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        # scale the values by their associated percentages and add them up\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Self-Attention using the above class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7802, -1.8837],\n",
       "        [-0.9534, -2.3194],\n",
       "        [-0.4130, -0.9592]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix of token encodings\n",
    "# To keep example simple we are considering the each encoding\n",
    "# to be a two digit encoding\n",
    "encoding_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                [0.57, 1.36],\n",
    "                                [4.41, -2.16]])\n",
    "\n",
    "# set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# create a basic self-attention object\n",
    "SelfAttention = SelfAttention(d_model = 2, \n",
    "                              row_dim = 0,\n",
    "                              col_dim = 1)\n",
    "\n",
    "# Attention score for the token embeddings supplied\n",
    "SelfAttention(encoding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results verification of the weights and overall calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Weight Matrix: tensor([[ 0.5406, -0.1657],\n",
      "        [ 0.5869,  0.6496]], grad_fn=<TransposeBackward0>)\n",
      "Key Weight Matrix: tensor([[ 0.6233,  0.6146],\n",
      "        [-0.5188,  0.1323]], grad_fn=<TransposeBackward0>)\n",
      "Value Weight Matrix: tensor([[-0.1549, -0.3443],\n",
      "        [ 0.1427,  0.4153]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Weight matrix for queries\n",
    "print(f'Query Weight Matrix: {SelfAttention.W_q.weight.transpose(0, 1)}')\n",
    "\n",
    "# Weight matrix for keys\n",
    "print(f'Key Weight Matrix: {SelfAttention.W_k.weight.transpose(0, 1)}')\n",
    "\n",
    "# Weight matrix for queries\n",
    "print(f'Value Weight Matrix: {SelfAttention.W_v.weight.transpose(0, 1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Matrix: tensor([[ 0.7621, -0.0428],\n",
      "        [ 1.1063,  0.7890],\n",
      "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)\n",
      "Key Matrix: tensor([[ 0.6038,  0.7434],\n",
      "        [-0.3502,  0.5303],\n",
      "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)\n",
      "Value Matrix: tensor([[-0.1469, -0.3038],\n",
      "        [ 0.1057,  0.3685],\n",
      "        [-0.9914, -2.4152]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculating the queries\n",
    "print(f'Query Matrix: {SelfAttention.W_q(encoding_matrix)}')\n",
    "\n",
    "# Calculating the keys\n",
    "print(f'Key Matrix: {SelfAttention.W_k(encoding_matrix)}')\n",
    "\n",
    "# Calculating the values\n",
    "print(f'Value Matrix: {SelfAttention.W_v(encoding_matrix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7621, -0.0428],\n",
       "        [ 1.1063,  0.7890],\n",
       "        [ 1.1164, -2.1336]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = SelfAttention.W_q(encoding_matrix)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6038,  0.7434],\n",
       "        [-0.3502,  0.5303],\n",
       "        [ 3.8695,  2.4246]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = SelfAttention.W_k(encoding_matrix)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4283, -0.2896,  2.8452],\n",
       "        [ 1.2545,  0.0310,  6.1939],\n",
       "        [-0.9121, -1.5224, -0.8533]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the unscaled similarity\n",
    "sims = torch.matmul(q, k.transpose(dim0=0, dim1=1))\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3029, -0.2048,  2.0119],\n",
       "        [ 0.8871,  0.0219,  4.3797],\n",
       "        [-0.6449, -1.0765, -0.6034]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the scaled similarity\n",
    "scaled_sims = sims / (torch.tensor(2)**0.5)\n",
    "scaled_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1403, 0.0845, 0.7752],\n",
       "        [0.0292, 0.0123, 0.9586],\n",
       "        [0.3715, 0.2413, 0.3872]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the attention percentages\n",
    "attention_percents = F.softmax(scaled_sims, dim=1)\n",
    "attention_percents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7802, -1.8837],\n",
       "        [-0.9534, -2.3194],\n",
       "        [-0.4130, -0.9592]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Attention Scores\n",
    "torch.matmul(attention_percents, SelfAttention.W_v(encoding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
