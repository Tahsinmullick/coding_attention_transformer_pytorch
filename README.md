# ğŸ”¥ Understanding Transformer Models & Matrix Multiplication ğŸ§®  

## ğŸš€ Objective  
This repository is dedicated to building the **core component** of Transformer modelsâ€”**Self-Attention**â€”from scratch in PyTorch. ğŸ¯  

Through this project, aim  is to deeply understand the **underlying matrix multiplication** that powers state-of-the-art attention mechanisms. ğŸ§   

---

## ğŸ§  Self-Attention from Scratch in PyTorch  
This implementation is inspired by the groundbreaking paper **"Attention Is All You Need"**, which introduced **Transformer-based models** to revolutionize NLP. ğŸ“âœ¨  

ğŸ“Œ **Key References:**  
- ğŸ”— [Research Paper on Attention Mechanisms](https://arxiv.org/abs/1706.03762) ğŸ“–  
- ğŸ“ Learning from the **DeepLearning.AI** course by **Josh Starmer**:  
  **Attention in Transformers: Concepts and Code in PyTorch** ğŸ“š  

---

## ğŸ”„ How Transformers Work  
At the heart of **Transformers** is the **Self-Attention** mechanism, which enables models to weigh the importance of different words in a sentence dynamically. âš¡  

ğŸ”¹ **Matrix Multiplication** is fundamental to computing attention scores, enabling efficient parallelization and scalability of Transformer models.  

ğŸ”¬ In this repo, we **implement** Self-Attention **from scratch**, breaking down concepts such as:  
- **Query, Key, Value Mechanisms** ğŸ”  
- **Scaled Dot-Product Attention** ğŸ”¢  
- **Multi-Head Attention** ğŸ†  

---
