# 🔥 Understanding Transformer Models & Matrix Multiplication 🧮  

## 🚀 Objective  
This repository is dedicated to building the **core component** of Transformer models—**Self-Attention**—from scratch in PyTorch. 🎯  

Through this project, aim  is to deeply understand the **underlying matrix multiplication** that powers state-of-the-art attention mechanisms. 🧠  

---

## 🧠 Self-Attention from Scratch in PyTorch  
This implementation is inspired by the groundbreaking paper **"Attention Is All You Need"**, which introduced **Transformer-based models** to revolutionize NLP. 📝✨  

📌 **Key References:**  
- 🔗 [Research Paper on Attention Mechanisms](https://arxiv.org/abs/1706.03762) 📖  
- 🎓 Learning from the **DeepLearning.AI** course by **Josh Starmer**:  
  **Attention in Transformers: Concepts and Code in PyTorch** 📚  

---

## 🔄 How Transformers Work  
At the heart of **Transformers** is the **Self-Attention** mechanism, which enables models to weigh the importance of different words in a sentence dynamically. ⚡  

🔹 **Matrix Multiplication** is fundamental to computing attention scores, enabling efficient parallelization and scalability of Transformer models.  

🔬 In this repo, we **implement** Self-Attention **from scratch**, breaking down concepts such as:  
- **Query, Key, Value Mechanisms** 🔍  
- **Scaled Dot-Product Attention** 🔢  
- **Multi-Head Attention** 🏆  

---
