## This repo is dedicated to creating the most integral part of the Transformer models and understanding the underlying matrix multiplication

### <u>Self-Attention from scratch in PyTorch</u>
- The idea behind this class is to implement a self-Attention Class similar to the **"Attention Is All You Need"** Paper [Research Paper on Attention Mechanisms](https://arxiv.org/abs/1706.03762) .
- It was great to learn how to code this from the DeepLearning.AI course by Josh Starmer Titled: **Attention in Transformers: Concepts and Code in PyTorch**
